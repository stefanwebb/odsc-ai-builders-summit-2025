{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from pymilvus import MilvusClient\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "class RAG:\n",
    "    \"\"\"\n",
    "    RAG (Retrieval-Augmented Generation) class built upon Milvus and HuggingFace + MLX.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm, embedding_model, milvus_client: MilvusClient):\n",
    "        self._llm = llm\n",
    "        self._embedding_model = embedding_model\n",
    "        self._prepare_milvus(milvus_client)\n",
    "\n",
    "    def _emb_text(self, text: str) -> List[float]: # np.floatarray, actually\n",
    "        return self._embedding_model.encode([text])[0]\n",
    "\n",
    "    def _prepare_milvus(\n",
    "        self, milvus_client: MilvusClient, collection_name: str = \"rag_collection\"\n",
    "    ):\n",
    "        self.milvus_client = milvus_client\n",
    "        self.collection_name = collection_name\n",
    "        if self.milvus_client.has_collection(self.collection_name):\n",
    "            self.milvus_client.drop_collection(self.collection_name)\n",
    "        embedding_dim = len(self._emb_text(\"foo\"))\n",
    "        self.milvus_client.create_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            dimension=embedding_dim,\n",
    "            metric_type=\"IP\",  # Inner product distance\n",
    "            consistency_level=\"Strong\",  # Strong consistency level\n",
    "        )\n",
    "\n",
    "    def load(self, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Load the text data into Milvus.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for i, line in enumerate(tqdm(texts, desc=\"Creating embeddings\")):\n",
    "            data.append({\"id\": i, \"vector\": self._emb_text(line), \"text\": line})\n",
    "\n",
    "        self.milvus_client.insert(collection_name=self.collection_name, data=data)\n",
    "\n",
    "    def retrieve(self, question: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieve the most similar text data to the given question.\n",
    "        \"\"\"\n",
    "        search_res = self.milvus_client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            data=[self._emb_text(question)],\n",
    "            limit=top_k,\n",
    "            search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n",
    "            output_fields=[\"text\"],  # Return the text field\n",
    "        )\n",
    "        retrieved_texts = [res[\"entity\"][\"text\"] for res in search_res[0]]\n",
    "        return retrieved_texts[:top_k]\n",
    "\n",
    "    def answer(\n",
    "        self,\n",
    "        question: str,\n",
    "        retrieval_top_k: int = 3,\n",
    "        return_retrieved_text: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Answer the given question with the retrieved knowledge.\n",
    "        \"\"\"\n",
    "        retrieved_texts = self.retrieve(question, top_k=retrieval_top_k)\n",
    "        \n",
    "        user_prompt = USER_PROMPT.format(\n",
    "            context=\"\\n\".join(retrieved_texts), question=question\n",
    "        )\n",
    "\n",
    "        # DEBUG\n",
    "        # print(user_prompt)\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=SYSTEM_PROMPT\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=user_prompt\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        res = self._llm.invoke(messages)\n",
    "        # print(res.content)\n",
    "\n",
    "        # response = self.openai_client.chat.completions.create(\n",
    "        #     model=self.llm_model,\n",
    "        #     messages=[\n",
    "        #         {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n",
    "        #         {\"role\": \"user\", \"content\": user_prompt},\n",
    "        #     ],\n",
    "        # )\n",
    "\n",
    "        if not return_retrieved_text:\n",
    "            return res.content\n",
    "        else:\n",
    "            return res.content, retrieved_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts and Open Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "\"\"\"\n",
    "USER_PROMPT = \"\"\"\n",
    "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_client = MilvusClient(uri=\"./milvus_demo2.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Embedding Model and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d659b13ccc445296558f866af23963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.models import StaticEmbedding\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# embedding_tokenizer = Tokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "# static_embedding = StaticEmbedding(embedding_tokenizer, embedding_dim=1024)\n",
    "# embedding_model = SentenceTransformer(modules=[static_embedding])\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "from mlx_lm import load\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/phi-4-4bit\")\n",
    "\n",
    "from langchain_community.llms.mlx_pipeline import MLXPipeline\n",
    "from langchain_community.chat_models.mlx import ChatMLX\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = MLXPipeline(\n",
    "    model=model, tokenizer=tokenizer, pipeline_kwargs={\"max_tokens\": 1024, \"temp\": 0.1}\n",
    ")\n",
    "\n",
    "chat = ChatMLX(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rag = RAG(llm=chat, embedding_model=embedding_model, milvus_client=milvus_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> 384 [-3.81771475e-02  3.29111144e-02 -5.45937615e-03  1.43699432e-02\n",
      " -4.02910337e-02 -1.16532452e-01  3.16876508e-02  1.91175763e-03\n",
      " -4.26223613e-02  2.91681066e-02  4.24266979e-02  3.20417061e-02\n",
      "  2.98447404e-02  1.09803099e-02 -5.39396591e-02 -5.02772704e-02\n",
      " -2.35078186e-02  1.07793650e-02 -1.37707949e-01  4.11500549e-03\n",
      "  2.93330774e-02  6.68411553e-02 -1.53894098e-02  4.84376550e-02\n",
      " -8.81497413e-02 -1.27268368e-02  4.14090268e-02  4.08314951e-02\n",
      " -5.01558892e-02 -5.81250526e-02  4.88014929e-02  6.88901097e-02\n",
      "  5.87469302e-02  8.73099361e-03 -1.59182679e-02  8.51419792e-02\n",
      " -7.81474486e-02 -7.75167719e-02  2.07237769e-02  1.61942448e-02\n",
      "  3.25105898e-02 -5.34888841e-02 -6.22287765e-02 -2.43146457e-02\n",
      "  7.41276331e-03  2.39777416e-02  6.36097370e-03  5.11451066e-02\n",
      "  7.27667212e-02  3.46497037e-02 -5.47711030e-02 -5.93284741e-02\n",
      " -7.16696167e-03  2.01377142e-02  3.58463563e-02  5.59091382e-03\n",
      "  1.07735554e-02 -5.27637303e-02  1.01473741e-02 -8.73163342e-03\n",
      " -6.28155321e-02  3.84666845e-02 -1.39427204e-02  7.35259056e-02\n",
      "  9.00083110e-02 -7.99807459e-02 -1.63944960e-02  4.47223857e-02\n",
      " -6.88312650e-02 -3.30075696e-02 -1.53512163e-02  1.12996995e-02\n",
      "  3.64983343e-02  6.62666932e-02 -5.44066839e-02  8.79519898e-03\n",
      "  1.20745711e-02 -3.81696336e-02  6.86011976e-03  5.11235893e-02\n",
      "  7.74175972e-02 -1.22962646e-01  1.63531266e-02  4.95427325e-02\n",
      "  3.17454189e-02 -3.96687724e-02  1.70920731e-03  9.66940541e-03\n",
      " -3.25298905e-02 -3.39295268e-02 -1.33264616e-01  7.39704724e-03\n",
      " -1.02342814e-02  3.85915153e-02 -9.33099091e-02 -4.16540392e-02\n",
      "  6.98687136e-02 -2.62886230e-02 -1.49698213e-01  1.34353131e-01\n",
      "  3.75027657e-02  5.28785363e-02  4.49671485e-02  1.85995288e-02\n",
      "  5.44252284e-02  1.72716416e-02 -3.25182937e-02  4.60801944e-02\n",
      " -4.67377342e-02 -3.06096729e-02 -1.82268936e-02 -4.86957841e-02\n",
      "  3.28579582e-02 -3.92094953e-03  5.01056351e-02 -5.82454763e-02\n",
      " -1.00809373e-02  1.05502727e-02 -4.01561037e-02 -1.55400450e-03\n",
      "  6.08768575e-02 -4.55005765e-02  4.92598563e-02  2.61243414e-02\n",
      "  1.98505875e-02 -1.58545387e-03  5.95724434e-02 -6.51835862e-33\n",
      "  6.35851771e-02  3.06810206e-03  2.88844611e-02  1.73389703e-01\n",
      "  2.99481279e-03  2.76897065e-02 -9.51482579e-02 -3.11607793e-02\n",
      "  2.66698860e-02 -1.08712874e-02  2.39151940e-02  2.38443743e-02\n",
      " -3.12165227e-02  4.94056381e-02 -2.49787867e-02  1.01824269e-01\n",
      " -7.92783275e-02 -3.24872974e-03  4.30462547e-02  9.49330330e-02\n",
      " -6.65736422e-02  6.32926356e-03  2.22788807e-02  6.99767917e-02\n",
      " -7.53039541e-03 -1.74521771e-03  2.70446949e-02 -7.53242746e-02\n",
      "  1.14057794e-01  8.55988543e-03 -2.36879569e-02 -4.69805077e-02\n",
      "  1.43719018e-02  1.98206306e-02 -4.58438089e-03  1.37420697e-03\n",
      " -3.43194194e-02 -5.41234091e-02 -9.41645876e-02 -2.89598089e-02\n",
      " -1.87951308e-02  4.58199345e-02  4.75889966e-02 -3.19492235e-03\n",
      " -3.32171768e-02 -1.33891162e-02  5.10315485e-02  3.10757700e-02\n",
      "  1.53144775e-02  5.42222820e-02 -8.50554407e-02  1.33044831e-02\n",
      " -4.78141643e-02  7.10234269e-02 -1.31599624e-02 -2.42902641e-03\n",
      "  5.02184406e-02 -4.16093990e-02 -1.41695086e-02  3.23881879e-02\n",
      "  5.37519297e-03  9.12262499e-02  4.55124537e-03 -1.83422510e-02\n",
      " -1.52029991e-02 -4.63723540e-02  3.86707969e-02  1.46849891e-02\n",
      "  5.20024970e-02  1.90895272e-03 -1.49174305e-02  2.70289648e-02\n",
      "  3.12726051e-02  2.36842204e-02 -4.80394624e-03  3.61618511e-02\n",
      "  6.67887330e-02 -1.89081416e-03  2.13742647e-02 -5.76926470e-02\n",
      "  1.91577263e-02  3.15590687e-02 -1.84470434e-02 -4.07710373e-02\n",
      "  1.03958391e-01  1.19074993e-02 -1.49294641e-02 -1.05078436e-01\n",
      " -1.23710036e-02 -3.03939771e-04 -9.50324908e-02  5.83013184e-02\n",
      "  4.26109396e-02 -2.50124019e-02 -9.46091563e-02  4.00341527e-33\n",
      "  1.32172972e-01  5.45831257e-03 -3.31512764e-02 -9.10781175e-02\n",
      " -3.15739624e-02 -3.38864066e-02 -7.19889551e-02  1.25911906e-01\n",
      " -8.33933651e-02  5.27782179e-02  1.12826703e-03  2.19778046e-02\n",
      "  1.04063600e-01  1.29887508e-02  4.08715680e-02  1.87307149e-02\n",
      "  1.14286281e-01  2.48653349e-02  1.46103837e-02  6.18681498e-03\n",
      " -1.13734845e-02 -3.57009768e-02 -3.80397476e-02  1.11748576e-02\n",
      " -5.12898676e-02  7.88672455e-03  6.72205240e-02  3.40405828e-03\n",
      " -9.28479508e-02  3.70485224e-02 -2.22388208e-02  4.00781445e-02\n",
      " -3.07131894e-02 -1.14214113e-02 -1.44187007e-02  2.50452403e-02\n",
      " -9.75571722e-02 -3.53958495e-02 -3.75209972e-02 -1.00683998e-02\n",
      " -6.38862103e-02  2.54725404e-02  2.06223968e-02  3.76771502e-02\n",
      " -1.04281679e-01 -2.82806568e-02 -5.21058217e-02  1.28588621e-02\n",
      " -5.14237210e-02 -2.90379841e-02 -9.63978544e-02 -4.23606001e-02\n",
      "  6.70596883e-02 -3.07707023e-02 -1.03944233e-02  2.74102651e-02\n",
      " -2.80103944e-02  1.02891978e-02  4.30914573e-02  2.22976841e-02\n",
      "  8.01136438e-03  5.61470911e-02  4.08860408e-02  9.28760469e-02\n",
      "  1.65849216e-02 -5.38247712e-02  5.74163278e-04  5.07842936e-02\n",
      "  4.24875207e-02 -2.92068999e-02  9.23895836e-03 -1.06735807e-02\n",
      " -3.71527672e-02  2.36695306e-03 -3.03456709e-02  7.45052695e-02\n",
      "  2.62970943e-03 -1.76091325e-02  2.82069179e-03  3.83743010e-02\n",
      "  7.22763222e-03  4.56757396e-02  4.00427431e-02  1.42476391e-02\n",
      " -1.43160727e-02  5.86555190e-02  3.63530628e-02  5.52346408e-02\n",
      " -1.99883785e-02 -8.04462060e-02 -3.02477293e-02 -1.49129592e-02\n",
      "  2.22688299e-02  1.19625637e-02 -6.91014454e-02 -1.88070981e-08\n",
      " -7.85505176e-02  4.67108525e-02 -2.40804981e-02  6.34382740e-02\n",
      "  2.40138676e-02  1.43956405e-03 -9.08353627e-02 -6.68759048e-02\n",
      " -8.00782442e-02  5.66643849e-03  5.36583886e-02  1.04866259e-01\n",
      " -6.68804571e-02  1.54984677e-02  6.71185181e-02  7.08869323e-02\n",
      " -3.19899805e-02  2.08834801e-02 -2.19384190e-02 -7.26884510e-03\n",
      " -1.08125852e-02  4.08996409e-03  3.31555009e-02 -7.89784491e-02\n",
      "  3.87152322e-02 -7.53202513e-02 -1.58048682e-02  5.96065400e-03\n",
      "  5.19900071e-03 -6.14302717e-02  4.20057885e-02  9.53627899e-02\n",
      " -4.32334617e-02  1.43934563e-02 -1.06087439e-01 -2.79941838e-02\n",
      "  1.09661706e-02  6.95306957e-02  6.69809803e-02 -7.47754723e-02\n",
      " -7.85745829e-02  4.27465700e-02 -3.46037485e-02 -1.06056273e-01\n",
      " -3.56334858e-02  5.15012741e-02  6.86736181e-02 -4.99744713e-02\n",
      "  1.52898533e-02 -6.45573512e-02 -7.59339407e-02  2.61542965e-02\n",
      "  7.42642134e-02 -1.24497693e-02  1.33297861e-01  7.47663453e-02\n",
      "  5.12522422e-02  2.09902953e-02 -2.68759932e-02  8.89062583e-02\n",
      "  4.00210880e-02 -4.08902951e-02  3.18714082e-02  1.81631818e-02]\n"
     ]
    }
   ],
   "source": [
    "v = my_rag._emb_text(\"Hello, world!\")\n",
    "print(type(v), len(v), v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pymilvus.milvus_client.milvus_client.MilvusClient object at 0x12bb65fd0>\n"
     ]
    }
   ],
   "source": [
    "print(my_rag.milvus_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rag_collection']\n"
     ]
    }
   ],
   "source": [
    "print(my_rag.milvus_client.list_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rag.retrieve(\"testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Vector Database with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|██████████| 47/47 [00:00<00:00, 76.79it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/milvus-io/milvus/master/DEVELOPMENT.md\"\n",
    "file_path = \"./Milvus_DEVELOPMENT.md\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "with open(file_path, \"r\") as file:\n",
    "    file_text = file.read()\n",
    "\n",
    "# We simply use \"# \" to separate the content in the file, which can roughly separate the content of each main part of the markdown file.\n",
    "text_lines = file_text.split(\"# \")\n",
    "my_rag.load(text_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hardware Requirements\\n\\nThe following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\\n\\n```\\n- 8GB of RAM\\n- 50GB of free disk space\\n```\\n\\n##',\n",
       " \"Software Requirements\\n\\nAll Linux distributions are available for Milvus development. However a majority of our contributor worked with Ubuntu or CentOS systems, with a small portion of Mac (both x86_64 and Apple Silicon) contributors. If you would like Milvus to build and run on other distributions, you are more than welcome to file an issue and contribute!\\n\\nHere's a list of verified OS types where Milvus can successfully build and run:\\n\\n- Debian/Ubuntu\\n- Amazon Linux\\n- MacOS (x86_64)\\n- MacOS (Apple Silicon)\\n\\n##\",\n",
       " 'Building Milvus on a local OS/shell environment\\n\\nThe details below outline the hardware and software requirements for building on Linux and MacOS.\\n\\n##']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is the hardware requirements specification if I want to build Milvus and run from source code?\"\n",
    "my_rag.retrieve(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Specifying sampling arguments to ``generate_step`` is deprecated. Pass in a ``sampler`` instead.\n",
      "To build and run Milvus from source code, the recommended hardware requirements are:\n",
      "\n",
      "- 8GB of RAM\n",
      "- 50GB of free disk space\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('To build and run Milvus from source code, the recommended hardware requirements are:\\n\\n- 8GB of RAM\\n- 50GB of free disk space',\n",
       " ['Hardware Requirements\\n\\nThe following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\\n\\n```\\n- 8GB of RAM\\n- 50GB of free disk space\\n```\\n\\n##',\n",
       "  \"Software Requirements\\n\\nAll Linux distributions are available for Milvus development. However a majority of our contributor worked with Ubuntu or CentOS systems, with a small portion of Mac (both x86_64 and Apple Silicon) contributors. If you would like Milvus to build and run on other distributions, you are more than welcome to file an issue and contribute!\\n\\nHere's a list of verified OS types where Milvus can successfully build and run:\\n\\n- Debian/Ubuntu\\n- Amazon Linux\\n- MacOS (x86_64)\\n- MacOS (Apple Silicon)\\n\\n##\",\n",
       "  'Building Milvus on a local OS/shell environment\\n\\nThe details below outline the hardware and software requirements for building on Linux and MacOS.\\n\\n##'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rag.answer(question, return_retrieved_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth\n",
    "Now let’s prepare some questions with its corresponding ground truth answers. We get answers and contexts from our RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering questions:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Specifying sampling arguments to ``generate_step`` is deprecated. Pass in a ``sampler`` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering questions:  33%|███▎      | 1/3 [00:08<00:16,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To build and run Milvus from source code, the recommended hardware specifications are:\n",
      "\n",
      "- 8GB of RAM\n",
      "- 50GB of free disk space\n",
      "\n",
      "These requirements are applicable for both physical and virtual machine resources.\n",
      "[Warning] Specifying sampling arguments to ``generate_step`` is deprecated. Pass in a ``sampler`` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering questions:  67%|██████▋   | 2/3 [00:12<00:05,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The programming language used to write Knowhere is C++. This information is found in the context where it states, \"The algorithm library of Milvus, Knowhere is written in C++.\"\n",
      "[Warning] Specifying sampling arguments to ``generate_step`` is deprecated. Pass in a ``sampler`` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering questions: 100%|██████████| 3/3 [00:17<00:00,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before running code coverage, it should be ensured that the code change is covered by unit tests. This is important to verify before submitting a pull request. The context specifies that developers should make sure their code change is covered by unit tests before proceeding with code coverage checks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is the hardware requirements specificatio...</td>\n",
       "      <td>[Hardware Requirements\\n\\nThe following specif...</td>\n",
       "      <td>To build and run Milvus from source code, the ...</td>\n",
       "      <td>If you want to build Milvus and run from sourc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the programming language used to write...</td>\n",
       "      <td>[Unless required by applicable law or agreed t...</td>\n",
       "      <td>The programming language used to write Knowher...</td>\n",
       "      <td>The programming language used to write Knowher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What should be ensured before running code cov...</td>\n",
       "      <td>[Code coverage\\n\\nBefore submitting your pull ...</td>\n",
       "      <td>Before running code coverage, it should be ens...</td>\n",
       "      <td>Before running code coverage, you should make ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  what is the hardware requirements specificatio...   \n",
       "1  What is the programming language used to write...   \n",
       "2  What should be ensured before running code cov...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Hardware Requirements\\n\\nThe following specif...   \n",
       "1  [Unless required by applicable law or agreed t...   \n",
       "2  [Code coverage\\n\\nBefore submitting your pull ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  To build and run Milvus from source code, the ...   \n",
       "1  The programming language used to write Knowher...   \n",
       "2  Before running code coverage, it should be ens...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  If you want to build Milvus and run from sourc...  \n",
       "1  The programming language used to write Knowher...  \n",
       "2  Before running code coverage, you should make ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "question_list = [\n",
    "    \"what is the hardware requirements specification if I want to build Milvus and run from source code?\",\n",
    "    \"What is the programming language used to write Knowhere?\",\n",
    "    \"What should be ensured before running code coverage?\",\n",
    "]\n",
    "ground_truth_list = [\n",
    "    \"If you want to build Milvus and run from source code, the recommended hardware requirements specification is:\\n\\n- 8GB of RAM\\n- 50GB of free disk space.\",\n",
    "    \"The programming language used to write Knowhere is C++.\",\n",
    "    \"Before running code coverage, you should make sure that your code changes are covered by unit tests.\",\n",
    "]\n",
    "contexts_list = []\n",
    "answer_list = []\n",
    "for question in tqdm(question_list, desc=\"Answering questions\"):\n",
    "    answer, contexts = my_rag.answer(question, return_retrieved_text=True)\n",
    "    contexts_list.append(contexts)\n",
    "    answer_list.append(answer)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": question_list,\n",
    "        \"contexts\": contexts_list,\n",
    "        \"answer\": answer_list,\n",
    "        \"ground_truth\": ground_truth_list,\n",
    "    }\n",
    ")\n",
    "rag_results = Dataset.from_pandas(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is the hardware requirements specification if I want to build Milvus and run from source code?',\n",
       " 'contexts': ['Hardware Requirements\\n\\nThe following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\\n\\n```\\n- 8GB of RAM\\n- 50GB of free disk space\\n```\\n\\n##',\n",
       "  \"Software Requirements\\n\\nAll Linux distributions are available for Milvus development. However a majority of our contributor worked with Ubuntu or CentOS systems, with a small portion of Mac (both x86_64 and Apple Silicon) contributors. If you would like Milvus to build and run on other distributions, you are more than welcome to file an issue and contribute!\\n\\nHere's a list of verified OS types where Milvus can successfully build and run:\\n\\n- Debian/Ubuntu\\n- Amazon Linux\\n- MacOS (x86_64)\\n- MacOS (Apple Silicon)\\n\\n##\",\n",
       "  'Building Milvus on a local OS/shell environment\\n\\nThe details below outline the hardware and software requirements for building on Linux and MacOS.\\n\\n##'],\n",
       " 'answer': 'To build and run Milvus from source code, the recommended hardware specifications are:\\n\\n- 8GB of RAM\\n- 50GB of free disk space\\n\\nThese requirements are applicable for both physical and virtual machine resources.',\n",
       " 'ground_truth': 'If you want to build Milvus and run from source code, the recommended hardware requirements specification is:\\n\\n- 8GB of RAM\\n- 50GB of free disk space.'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'StringPromptValue' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m sample \u001b[38;5;241m=\u001b[39m SingleTurnSample(\n\u001b[1;32m      5\u001b[0m         user_input\u001b[38;5;241m=\u001b[39mrag_results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m         response\u001b[38;5;241m=\u001b[39mrag_results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m         retrieved_contexts\u001b[38;5;241m=\u001b[39mrag_results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontexts\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m scorer \u001b[38;5;241m=\u001b[39m Faithfulness(llm\u001b[38;5;241m=\u001b[39mchat)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m scorer\u001b[38;5;241m.\u001b[39msingle_turn_ascore(sample)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/milvus/lib/python3.12/site-packages/ragas/metrics/base.py:541\u001b[0m, in \u001b[0;36mSingleTurnMetric.single_turn_ascore\u001b[0;34m(self, sample, callbacks, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[1;32m    540\u001b[0m         rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/milvus/lib/python3.12/site-packages/ragas/metrics/base.py:534\u001b[0m, in \u001b[0;36mSingleTurnMetric.single_turn_ascore\u001b[0;34m(self, sample, callbacks, timeout)\u001b[0m\n\u001b[1;32m    527\u001b[0m rm, group_cm \u001b[38;5;241m=\u001b[39m new_group(\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    529\u001b[0m     inputs\u001b[38;5;241m=\u001b[39msample\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m    530\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    531\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: ChainType\u001b[38;5;241m.\u001b[39mMETRIC},\n\u001b[1;32m    532\u001b[0m )\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(\n\u001b[1;32m    535\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_turn_ascore(sample\u001b[38;5;241m=\u001b[39msample, callbacks\u001b[38;5;241m=\u001b[39mgroup_cm),\n\u001b[1;32m    536\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    537\u001b[0m     )\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/milvus/lib/python3.12/asyncio/tasks.py:520\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m timeouts\u001b[38;5;241m.\u001b[39mtimeout(timeout):\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n",
      "File \u001b[0;32m/opt/miniconda3/envs/milvus/lib/python3.12/site-packages/ragas/metrics/_faithfulness.py:200\u001b[0m, in \u001b[0;36mFaithfulness._single_turn_ascore\u001b[0;34m(self, sample, callbacks)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_single_turn_ascore\u001b[39m(\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m, sample: SingleTurnSample, callbacks: Callbacks\n\u001b[1;32m    198\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    199\u001b[0m     row \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ascore(row, callbacks)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/milvus/lib/python3.12/site-packages/ragas/metrics/_faithfulness.py:208\u001b[0m, in \u001b[0;36mFaithfulness._ascore\u001b[0;34m(self, row, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03mreturns the NLI score for each (q, c, a) pair\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM is not set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 208\u001b[0m statements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_statements(row, callbacks)\n\u001b[1;32m    209\u001b[0m statements \u001b[38;5;241m=\u001b[39m statements\u001b[38;5;241m.\u001b[39mstatements\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m statements \u001b[38;5;241m==\u001b[39m []:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/milvus/lib/python3.12/site-packages/ragas/metrics/_faithfulness.py:174\u001b[0m, in \u001b[0;36mFaithfulness._create_statements\u001b[0;34m(self, row, callbacks)\u001b[0m\n\u001b[1;32m    171\u001b[0m text, question \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m], row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    173\u001b[0m prompt_input \u001b[38;5;241m=\u001b[39m StatementGeneratorInput(question\u001b[38;5;241m=\u001b[39mquestion, answer\u001b[38;5;241m=\u001b[39mtext)\n\u001b[0;32m--> 174\u001b[0m statements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatement_generator_prompt\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    175\u001b[0m     llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm,\n\u001b[1;32m    176\u001b[0m     data\u001b[38;5;241m=\u001b[39mprompt_input,\n\u001b[1;32m    177\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    178\u001b[0m )\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m statements\n",
      "File \u001b[0;32m/opt/miniconda3/envs/milvus/lib/python3.12/site-packages/ragas/prompt/pydantic_prompt.py:127\u001b[0m, in \u001b[0;36mPydanticPrompt.generate\u001b[0;34m(self, llm, data, temperature, stop, callbacks, retries_left)\u001b[0m\n\u001b[1;32m    124\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m callbacks \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# this is just a special case of generate_multiple\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m output_single \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_multiple(\n\u001b[1;32m    128\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    129\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    130\u001b[0m     n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    131\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    132\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    133\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    134\u001b[0m     retries_left\u001b[38;5;241m=\u001b[39mretries_left,\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_single[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/milvus/lib/python3.12/site-packages/ragas/prompt/pydantic_prompt.py:188\u001b[0m, in \u001b[0;36mPydanticPrompt.generate_multiple\u001b[0;34m(self, llm, data, n, temperature, stop, callbacks, retries_left)\u001b[0m\n\u001b[1;32m    181\u001b[0m prompt_rm, prompt_cb \u001b[38;5;241m=\u001b[39m new_group(\n\u001b[1;32m    182\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    183\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: processed_data},\n\u001b[1;32m    184\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    185\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: ChainType\u001b[38;5;241m.\u001b[39mRAGAS_PROMPT},\n\u001b[1;32m    186\u001b[0m )\n\u001b[1;32m    187\u001b[0m prompt_value \u001b[38;5;241m=\u001b[39m PromptValue(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_string(processed_data))\n\u001b[0;32m--> 188\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_cb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m output_models \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    197\u001b[0m parser \u001b[38;5;241m=\u001b[39m RagasOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_model)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/milvus/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:627\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m inheritable_metadata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(metadata \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ls_params(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    609\u001b[0m }\n\u001b[1;32m    611\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m CallbackManager\u001b[38;5;241m.\u001b[39mconfigure(\n\u001b[1;32m    612\u001b[0m     callbacks,\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    619\u001b[0m )\n\u001b[1;32m    620\u001b[0m run_managers \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chat_model_start(\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m    622\u001b[0m     messages,\n\u001b[1;32m    623\u001b[0m     invocation_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    624\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    625\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    626\u001b[0m     run_id\u001b[38;5;241m=\u001b[39mrun_id,\n\u001b[0;32m--> 627\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    628\u001b[0m )\n\u001b[1;32m    629\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'StringPromptValue' has no len()"
     ]
    }
   ],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample \n",
    "from ragas.metrics import Faithfulness\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "        user_input=rag_results[0]['question'],\n",
    "        response=rag_results[0]['answer'],\n",
    "        retrieved_contexts=rag_results[0]['contexts']\n",
    "    )\n",
    "scorer = Faithfulness(llm=chat)\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milvus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
