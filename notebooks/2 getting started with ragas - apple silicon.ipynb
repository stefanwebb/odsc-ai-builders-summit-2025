{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.models import StaticEmbedding\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_tokenizer = Tokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "static_embedding = StaticEmbedding(embedding_tokenizer, embedding_dim=1024)\n",
    "embedding_model = SentenceTransformer(modules=[static_embedding])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load (quantized) Phi-4 for Metal (Mac Sillicon) hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5313a9f8964a4296a43bd4638ff4c522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlx_lm import load\n",
    "model, tokenizer = load(\"mlx-community/phi-4-4bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integration with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.mlx_pipeline import MLXPipeline\n",
    "from langchain_community.chat_models.mlx import ChatMLX\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = MLXPipeline(\n",
    "    model=model, tokenizer=tokenizer, pipeline_kwargs={\"max_tokens\": 1024, \"temp\": 0.1}\n",
    ")\n",
    "\n",
    "chat = ChatMLX(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Specifying sampling arguments to ``generate_step`` is deprecated. Pass in a ``sampler`` instead.\n",
      "As an AI, I can provide some perspectives on this classic paradox, but it's important to note that it is a thought experiment that doesn't have a definitive answer within the framework of our current understanding of physics.\n",
      "\n",
      "The paradox of an \"unstoppable force\" meeting an \"immovable object\" presents a logical contradiction: if a force is truly unstoppable, it should be able to move any object, no matter how immovable it seems. Conversely, if an object is truly immovable, no force should be able to move it. This creates a scenario where the two definitions cannot coexist without violating the principles of logic or physics as we understand them.\n",
      "\n",
      "In theoretical discussions, this paradox is often used to explore the limits of language, logic, and the nature of reality. It can also serve as a metaphor for situations where two seemingly irreconcilable forces or ideas come into conflict.\n",
      "\n",
      "Ultimately, the question is more philosophical than scientific, as it challenges the definitions and assumptions we make about the universe. Without additional context or a redefinition of the terms involved, the paradox remains an intriguing but unsolvable puzzle.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# for chunk in chat.stream(messages):\n",
    "#     print(chunk)\n",
    "res = chat.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ragas wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = LangchainLLMWrapper(chat)\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Using an LLM-based Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Specifying sampling arguments to ``generate_step`` is deprecated. Pass in a ``sampler`` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import AspectCritic\n",
    "\n",
    "test_data = {\n",
    "    \"user_input\": \"summarise given text\\nThe company reported an 8% rise in Q3 2024, driven by strong performance in the Asian market. Sales in this region have significantly contributed to the overall growth. Analysts attribute this success to strategic marketing and product localization. The positive trend in the Asian market is expected to continue into the next quarter.\",\n",
    "    \"response\": \"The company experienced an 8% increase in Q3 2024, largely due to effective marketing strategies and product adaptation, with expectations of continued growth in the coming quarter.\",\n",
    "}\n",
    "\n",
    "metric = AspectCritic(\n",
    "    name=\"summary_accuracy\",\n",
    "    llm=evaluator_llm,\n",
    "    definition=\"Verify if the summary is accurate.\",\n",
    ")\n",
    "test_data = SingleTurnSample(**test_data)\n",
    "await metric.single_turn_ascore(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating on a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in dataset: ['user_input', 'response']\n",
      "Total samples in dataset: 3\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "\n",
    "eval_dataset = load_dataset(\n",
    "    \"explodinggradients/earning_report_summary\", split=\"train\"\n",
    ").take(3)\n",
    "eval_dataset = EvaluationDataset.from_hf_dataset(eval_dataset)\n",
    "print(\"Features in dataset:\", eval_dataset.features())\n",
    "print(\"Total samples in dataset:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630297e417fe455ba6c957e4c4d75e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Specifying sampling arguments to ``generate_step`` is deprecated. Pass in a ``sampler`` instead.[Warning] Specifying sampling arguments to ``generate_step`` is deprecated. Pass in a ``sampler`` instead.\n",
      "[Warning] Specifying sampling arguments to ``generate_step`` is deprecated. Pass in a ``sampler`` instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary_accuracy': 1.0000}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate(eval_dataset, metrics=[metric])\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milvus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
